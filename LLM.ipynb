{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 21918\n",
      "1The Verdict\n",
      "Edith Wharton\n",
      "1908\n",
      "Exported from Wikisource on May 20, 20242I HAD always thought Jack \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import PyPDF2\n",
    "with open(r\"C:\\Users\\utkar\\OneDrive\\Desktop\\CAO PYQS\\The_Verdict.pdf\", \"rb\") as f:\n",
    "    pdf_reader = PyPDF2.PdfReader(f)\n",
    "    raw_text = \"\"\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        raw_text += page.extract_text()\n",
    "    \n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "# Strip whitespace from each item and then filter out any empty strings.\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1The', 'Verdict', 'Edith', 'Wharton', '1908', 'Exported', 'from', 'Wikisource', 'on', 'May', '20', ',', '20242I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4942\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('-colour', 7)\n",
      "('-presses', 8)\n",
      "('.', 9)\n",
      "('0', 10)\n",
      "('1', 11)\n",
      "('10But', 12)\n",
      "('14', 13)\n",
      "('19', 14)\n",
      "('1908', 15)\n",
      "('1929', 16)\n",
      "('1The', 17)\n",
      "('1above', 18)\n",
      "('20', 19)\n",
      "('2021About', 20)\n",
      "('20242I', 21)\n",
      "('4', 22)\n",
      "('5', 23)\n",
      "(':', 24)\n",
      "(';', 25)\n",
      "('?', 26)\n",
      "('A', 27)\n",
      "('Abigor', 28)\n",
      "('AdamBMor', 29)\n",
      "('Ah', 30)\n",
      "('Among', 31)\n",
      "('And', 32)\n",
      "('Are', 33)\n",
      "('Arrt', 34)\n",
      "('As', 35)\n",
      "('At', 36)\n",
      "('Attribution-ShareAlike', 37)\n",
      "('AzaT', 38)\n",
      "('Be', 39)\n",
      "('Begin', 40)\n",
      "('Bender235', 41)\n",
      "('Blurpeace', 42)\n",
      "('Boris23', 43)\n",
      "('Bromskloss', 44)\n",
      "('Burlington', 45)\n",
      "('But', 46)\n",
      "('By', 47)\n",
      "('Carlo', 48)\n",
      "('Chicago', 49)\n",
      "('Claude', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 91, 2, 976, 1120, 701, 627, 865, 5, 1271, 693, 5, 1, 107, 9, 72, 977, 1251, 873, 913, 9]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.32.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/10.5 MB 74.9 kB/s eta 0:02:13\n",
      "   -- ------------------------------------- 0.5/10.5 MB 74.9 kB/s eta 0:02:13\n",
      "   -- ------------------------------------- 0.5/10.5 MB 74.9 kB/s eta 0:02:13\n",
      "   -- ------------------------------------- 0.5/10.5 MB 74.9 kB/s eta 0:02:13\n",
      "   -- ------------------------------------- 0.5/10.5 MB 74.9 kB/s eta 0:02:13\n",
      "   -- ------------------------------------- 0.5/10.5 MB 74.9 kB/s eta 0:02:13\n",
      "   -- ------------------------------------- 0.5/10.5 MB 74.9 kB/s eta 0:02:13\n",
      "   -- ------------------------------------- 0.5/10.5 MB 74.9 kB/s eta 0:02:13\n",
      "   -- ------------------------------------- 0.5/10.5 MB 74.9 kB/s eta 0:02:13\n",
      "   -- ------------------------------------- 0.5/10.5 MB 74.9 kB/s eta 0:02:13\n",
      "   -- ------------------------------------- 0.5/10.5 MB 74.9 kB/s eta 0:02:13\n",
      "   -- ------------------------------------- 0.5/10.5 MB 74.9 kB/s eta 0:02:13\n",
      "   --- ------------------------------------ 0.8/10.5 MB 85.2 kB/s eta 0:01:54\n",
      "   --- ------------------------------------ 0.8/10.5 MB 85.2 kB/s eta 0:01:54\n",
      "   --- ------------------------------------ 0.8/10.5 MB 85.2 kB/s eta 0:01:54\n",
      "   --- ------------------------------------ 0.8/10.5 MB 85.2 kB/s eta 0:01:54\n",
      "   --- ------------------------------------ 0.8/10.5 MB 85.2 kB/s eta 0:01:54\n",
      "   --- ------------------------------------ 0.8/10.5 MB 85.2 kB/s eta 0:01:54\n",
      "   --- ------------------------------------ 0.8/10.5 MB 85.2 kB/s eta 0:01:54\n",
      "   --- ------------------------------------ 0.8/10.5 MB 85.2 kB/s eta 0:01:54\n",
      "   --- ------------------------------------ 0.8/10.5 MB 85.2 kB/s eta 0:01:54\n",
      "   --- ------------------------------------ 0.8/10.5 MB 85.2 kB/s eta 0:01:54\n",
      "   --- ------------------------------------ 0.8/10.5 MB 85.2 kB/s eta 0:01:54\n",
      "   --- ------------------------------------ 0.8/10.5 MB 85.2 kB/s eta 0:01:54\n",
      "   --- ------------------------------------ 0.8/10.5 MB 85.2 kB/s eta 0:01:54\n",
      "   --- ------------------------------------ 0.8/10.5 MB 85.2 kB/s eta 0:01:54\n",
      "   --- ------------------------------------ 0.8/10.5 MB 85.2 kB/s eta 0:01:54\n",
      "   --- ------------------------------------ 0.8/10.5 MB 85.2 kB/s eta 0:01:54\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 1.0/10.5 MB 84.0 kB/s eta 0:01:53\n",
      "   ----- ---------------------------------- 1.3/10.5 MB 76.6 kB/s eta 0:02:00\n",
      "   ----- ---------------------------------- 1.3/10.5 MB 76.6 kB/s eta 0:02:00\n",
      "   ----- ---------------------------------- 1.3/10.5 MB 76.6 kB/s eta 0:02:00\n",
      "   ----- ---------------------------------- 1.3/10.5 MB 76.6 kB/s eta 0:02:00\n",
      "   ----- ---------------------------------- 1.3/10.5 MB 76.6 kB/s eta 0:02:00\n",
      "   ----- ---------------------------------- 1.3/10.5 MB 76.6 kB/s eta 0:02:00\n",
      "   ----- ---------------------------------- 1.3/10.5 MB 76.6 kB/s eta 0:02:00\n",
      "   ------ --------------------------------- 1.6/10.5 MB 86.8 kB/s eta 0:01:43\n",
      "   ------ --------------------------------- 1.6/10.5 MB 86.8 kB/s eta 0:01:43\n",
      "   ------ --------------------------------- 1.6/10.5 MB 86.8 kB/s eta 0:01:43\n",
      "   ------ --------------------------------- 1.6/10.5 MB 86.8 kB/s eta 0:01:43\n",
      "   ------ --------------------------------- 1.6/10.5 MB 86.8 kB/s eta 0:01:43\n",
      "   ------ --------------------------------- 1.6/10.5 MB 86.8 kB/s eta 0:01:43\n",
      "   ------ --------------------------------- 1.6/10.5 MB 86.8 kB/s eta 0:01:43\n",
      "   ------ --------------------------------- 1.6/10.5 MB 86.8 kB/s eta 0:01:43\n",
      "   ------ --------------------------------- 1.6/10.5 MB 86.8 kB/s eta 0:01:43\n",
      "   ------ --------------------------------- 1.6/10.5 MB 86.8 kB/s eta 0:01:43\n",
      "   ------ --------------------------------- 1.6/10.5 MB 86.8 kB/s eta 0:01:43\n",
      "   ------ --------------------------------- 1.6/10.5 MB 86.8 kB/s eta 0:01:43\n",
      "   ------ --------------------------------- 1.6/10.5 MB 86.8 kB/s eta 0:01:43\n",
      "   ------ --------------------------------- 1.6/10.5 MB 86.8 kB/s eta 0:01:43\n",
      "   ------ --------------------------------- 1.6/10.5 MB 86.8 kB/s eta 0:01:43\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   ------- -------------------------------- 1.8/10.5 MB 86.0 kB/s eta 0:01:41\n",
      "   -------- ------------------------------- 2.1/10.5 MB 81.6 kB/s eta 0:01:43\n",
      "   -------- ------------------------------- 2.1/10.5 MB 81.6 kB/s eta 0:01:43\n",
      "   -------- ------------------------------- 2.1/10.5 MB 81.6 kB/s eta 0:01:43\n",
      "   -------- ------------------------------- 2.1/10.5 MB 81.6 kB/s eta 0:01:43\n",
      "   -------- ------------------------------- 2.1/10.5 MB 81.6 kB/s eta 0:01:43\n",
      "   -------- ------------------------------- 2.1/10.5 MB 81.6 kB/s eta 0:01:43\n",
      "   -------- ------------------------------- 2.1/10.5 MB 81.6 kB/s eta 0:01:43\n",
      "   -------- ------------------------------- 2.1/10.5 MB 81.6 kB/s eta 0:01:43\n",
      "   -------- ------------------------------- 2.1/10.5 MB 81.6 kB/s eta 0:01:43\n",
      "   -------- ------------------------------- 2.1/10.5 MB 81.6 kB/s eta 0:01:43\n",
      "   -------- ------------------------------- 2.1/10.5 MB 81.6 kB/s eta 0:01:43\n",
      "   -------- ------------------------------- 2.1/10.5 MB 81.6 kB/s eta 0:01:43\n",
      "   --------- ------------------------------ 2.4/10.5 MB 83.4 kB/s eta 0:01:38\n",
      "   --------- ------------------------------ 2.4/10.5 MB 83.4 kB/s eta 0:01:38\n",
      "   --------- ------------------------------ 2.4/10.5 MB 83.4 kB/s eta 0:01:38\n",
      "   --------- ------------------------------ 2.4/10.5 MB 83.4 kB/s eta 0:01:38\n",
      "   --------- ------------------------------ 2.4/10.5 MB 83.4 kB/s eta 0:01:38\n",
      "   --------- ------------------------------ 2.4/10.5 MB 83.4 kB/s eta 0:01:38\n",
      "   --------- ------------------------------ 2.4/10.5 MB 83.4 kB/s eta 0:01:38\n",
      "   --------- ------------------------------ 2.4/10.5 MB 83.4 kB/s eta 0:01:38\n",
      "   --------- ------------------------------ 2.4/10.5 MB 83.4 kB/s eta 0:01:38\n",
      "   --------- ------------------------------ 2.4/10.5 MB 83.4 kB/s eta 0:01:38\n",
      "   --------- ------------------------------ 2.4/10.5 MB 83.4 kB/s eta 0:01:38\n",
      "   --------- ------------------------------ 2.4/10.5 MB 83.4 kB/s eta 0:01:38\n",
      "   --------- ------------------------------ 2.4/10.5 MB 83.4 kB/s eta 0:01:38\n",
      "   --------- ------------------------------ 2.4/10.5 MB 83.4 kB/s eta 0:01:38\n",
      "   --------- ------------------------------ 2.4/10.5 MB 83.4 kB/s eta 0:01:38\n",
      "   --------- ------------------------------ 2.4/10.5 MB 83.4 kB/s eta 0:01:38\n",
      "   --------- ------------------------------ 2.4/10.5 MB 83.4 kB/s eta 0:01:38\n",
      "   ---------- ----------------------------- 2.6/10.5 MB 82.2 kB/s eta 0:01:36\n",
      "   ---------- ----------------------------- 2.6/10.5 MB 82.2 kB/s eta 0:01:36\n",
      "   ---------- ----------------------------- 2.6/10.5 MB 82.2 kB/s eta 0:01:36\n",
      "   ---------- ----------------------------- 2.6/10.5 MB 82.2 kB/s eta 0:01:36\n",
      "   ---------- ----------------------------- 2.6/10.5 MB 82.2 kB/s eta 0:01:36\n",
      "   ---------- ----------------------------- 2.6/10.5 MB 82.2 kB/s eta 0:01:36\n",
      "   ---------- ----------------------------- 2.6/10.5 MB 82.2 kB/s eta 0:01:36\n",
      "   ---------- ----------------------------- 2.6/10.5 MB 82.2 kB/s eta 0:01:36\n",
      "   ---------- ----------------------------- 2.6/10.5 MB 82.2 kB/s eta 0:01:36\n",
      "   ---------- ----------------------------- 2.6/10.5 MB 82.2 kB/s eta 0:01:36\n",
      "   ---------- ----------------------------- 2.6/10.5 MB 82.2 kB/s eta 0:01:36\n",
      "   ---------- ----------------------------- 2.6/10.5 MB 82.2 kB/s eta 0:01:36\n",
      "   ---------- ----------------------------- 2.6/10.5 MB 82.2 kB/s eta 0:01:36\n",
      "   ----------- ---------------------------- 2.9/10.5 MB 84.4 kB/s eta 0:01:30\n",
      "   ----------- ---------------------------- 2.9/10.5 MB 84.4 kB/s eta 0:01:30\n",
      "   ----------- ---------------------------- 2.9/10.5 MB 84.4 kB/s eta 0:01:30\n",
      "   ----------- ---------------------------- 2.9/10.5 MB 84.4 kB/s eta 0:01:30\n",
      "   ----------- ---------------------------- 2.9/10.5 MB 84.4 kB/s eta 0:01:30\n",
      "   ----------- ---------------------------- 2.9/10.5 MB 84.4 kB/s eta 0:01:30\n",
      "   ----------- ---------------------------- 2.9/10.5 MB 84.4 kB/s eta 0:01:30\n",
      "   ----------- ---------------------------- 2.9/10.5 MB 84.4 kB/s eta 0:01:30\n",
      "   ----------- ---------------------------- 2.9/10.5 MB 84.4 kB/s eta 0:01:30\n",
      "   ------------ --------------------------- 3.1/10.5 MB 88.0 kB/s eta 0:01:24\n",
      "   ------------ --------------------------- 3.1/10.5 MB 88.0 kB/s eta 0:01:24\n",
      "   ------------ --------------------------- 3.1/10.5 MB 88.0 kB/s eta 0:01:24\n",
      "   ------------ --------------------------- 3.1/10.5 MB 88.0 kB/s eta 0:01:24\n",
      "   ------------ --------------------------- 3.1/10.5 MB 88.0 kB/s eta 0:01:24\n",
      "   ------------ --------------------------- 3.1/10.5 MB 88.0 kB/s eta 0:01:24\n",
      "   ------------ --------------------------- 3.1/10.5 MB 88.0 kB/s eta 0:01:24\n",
      "   ------------ --------------------------- 3.1/10.5 MB 88.0 kB/s eta 0:01:24\n",
      "   ------------ --------------------------- 3.1/10.5 MB 88.0 kB/s eta 0:01:24\n",
      "   ------------ --------------------------- 3.1/10.5 MB 88.0 kB/s eta 0:01:24\n",
      "   ------------ --------------------------- 3.1/10.5 MB 88.0 kB/s eta 0:01:24\n",
      "   ------------ --------------------------- 3.1/10.5 MB 88.0 kB/s eta 0:01:24\n",
      "   ------------- -------------------------- 3.4/10.5 MB 88.1 kB/s eta 0:01:21\n",
      "   ------------- -------------------------- 3.4/10.5 MB 88.1 kB/s eta 0:01:21\n",
      "   ------------- -------------------------- 3.4/10.5 MB 88.1 kB/s eta 0:01:21\n",
      "   ------------- -------------------------- 3.4/10.5 MB 88.1 kB/s eta 0:01:21\n",
      "   ------------- -------------------------- 3.4/10.5 MB 88.1 kB/s eta 0:01:21\n",
      "   ------------- -------------------------- 3.4/10.5 MB 88.1 kB/s eta 0:01:21\n",
      "   ------------- -------------------------- 3.4/10.5 MB 88.1 kB/s eta 0:01:21\n",
      "   ------------- -------------------------- 3.4/10.5 MB 88.1 kB/s eta 0:01:21\n",
      "   ------------- -------------------------- 3.4/10.5 MB 88.1 kB/s eta 0:01:21\n",
      "   ------------- -------------------------- 3.4/10.5 MB 88.1 kB/s eta 0:01:21\n",
      "   ------------- -------------------------- 3.4/10.5 MB 88.1 kB/s eta 0:01:21\n",
      "   ------------- -------------------------- 3.4/10.5 MB 88.1 kB/s eta 0:01:21\n",
      "   ------------- -------------------------- 3.4/10.5 MB 88.1 kB/s eta 0:01:21\n",
      "   ------------- -------------------------- 3.4/10.5 MB 88.1 kB/s eta 0:01:21\n",
      "   ------------- -------------------------- 3.4/10.5 MB 88.1 kB/s eta 0:01:21\n",
      "   -------------- ------------------------- 3.7/10.5 MB 88.8 kB/s eta 0:01:17\n",
      "   -------------- ------------------------- 3.7/10.5 MB 88.8 kB/s eta 0:01:17\n",
      "   -------------- ------------------------- 3.7/10.5 MB 88.8 kB/s eta 0:01:17\n",
      "   -------------- ------------------------- 3.7/10.5 MB 88.8 kB/s eta 0:01:17\n",
      "   -------------- ------------------------- 3.7/10.5 MB 88.8 kB/s eta 0:01:17\n",
      "   -------------- ------------------------- 3.7/10.5 MB 88.8 kB/s eta 0:01:17\n",
      "   -------------- ------------------------- 3.7/10.5 MB 88.8 kB/s eta 0:01:17\n",
      "   -------------- ------------------------- 3.7/10.5 MB 88.8 kB/s eta 0:01:17\n",
      "   --------------- ------------------------ 3.9/10.5 MB 97.1 kB/s eta 0:01:08\n",
      "   --------------- ------------------------ 3.9/10.5 MB 97.1 kB/s eta 0:01:08\n",
      "   --------------- ------------------------ 3.9/10.5 MB 97.1 kB/s eta 0:01:08\n",
      "   --------------- ------------------------ 3.9/10.5 MB 97.1 kB/s eta 0:01:08\n",
      "   --------------- ------------------------ 3.9/10.5 MB 97.1 kB/s eta 0:01:08\n",
      "   --------------- ------------------------ 3.9/10.5 MB 97.1 kB/s eta 0:01:08\n",
      "   --------------- ------------------------ 3.9/10.5 MB 97.1 kB/s eta 0:01:08\n",
      "   ---------------- ----------------------- 4.2/10.5 MB 101.5 kB/s eta 0:01:02\n",
      "   ---------------- ----------------------- 4.2/10.5 MB 101.5 kB/s eta 0:01:02\n",
      "   ---------------- ----------------------- 4.2/10.5 MB 101.5 kB/s eta 0:01:02\n",
      "   ---------------- ----------------------- 4.2/10.5 MB 101.5 kB/s eta 0:01:02\n",
      "   ----------------- ---------------------- 4.5/10.5 MB 107.1 kB/s eta 0:00:57\n",
      "   ----------------- ---------------------- 4.5/10.5 MB 107.1 kB/s eta 0:00:57\n",
      "   ----------------- ---------------------- 4.5/10.5 MB 107.1 kB/s eta 0:00:57\n",
      "   ----------------- ---------------------- 4.5/10.5 MB 107.1 kB/s eta 0:00:57\n",
      "   ----------------- ---------------------- 4.5/10.5 MB 107.1 kB/s eta 0:00:57\n",
      "   ----------------- ---------------------- 4.5/10.5 MB 107.1 kB/s eta 0:00:57\n",
      "   ----------------- ---------------------- 4.5/10.5 MB 107.1 kB/s eta 0:00:57\n",
      "   ----------------- ---------------------- 4.5/10.5 MB 107.1 kB/s eta 0:00:57\n",
      "   ----------------- ---------------------- 4.5/10.5 MB 107.1 kB/s eta 0:00:57\n",
      "   ----------------- ---------------------- 4.5/10.5 MB 107.1 kB/s eta 0:00:57\n",
      "   ----------------- ---------------------- 4.5/10.5 MB 107.1 kB/s eta 0:00:57\n",
      "   ----------------- ---------------------- 4.5/10.5 MB 107.1 kB/s eta 0:00:57\n",
      "   ------------------ --------------------- 4.7/10.5 MB 106.2 kB/s eta 0:00:55\n",
      "   ------------------ --------------------- 4.7/10.5 MB 106.2 kB/s eta 0:00:55\n",
      "   ------------------ --------------------- 4.7/10.5 MB 106.2 kB/s eta 0:00:55\n",
      "   ------------------ --------------------- 4.7/10.5 MB 106.2 kB/s eta 0:00:55\n",
      "   ------------------ --------------------- 4.7/10.5 MB 106.2 kB/s eta 0:00:55\n",
      "   ------------------ --------------------- 4.7/10.5 MB 106.2 kB/s eta 0:00:55\n",
      "   ------------------ --------------------- 4.7/10.5 MB 106.2 kB/s eta 0:00:55\n",
      "   ------------------ --------------------- 4.7/10.5 MB 106.2 kB/s eta 0:00:55\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   ------------------- -------------------- 5.0/10.5 MB 108.8 kB/s eta 0:00:51\n",
      "   -------------------- ------------------- 5.2/10.5 MB 106.3 kB/s eta 0:00:50\n",
      "   -------------------- ------------------- 5.2/10.5 MB 106.3 kB/s eta 0:00:50\n",
      "   -------------------- ------------------- 5.2/10.5 MB 106.3 kB/s eta 0:00:50\n",
      "   -------------------- ------------------- 5.2/10.5 MB 106.3 kB/s eta 0:00:50\n",
      "   -------------------- ------------------- 5.2/10.5 MB 106.3 kB/s eta 0:00:50\n",
      "   -------------------- ------------------- 5.2/10.5 MB 106.3 kB/s eta 0:00:50\n",
      "   -------------------- ------------------- 5.2/10.5 MB 106.3 kB/s eta 0:00:50\n",
      "   -------------------- ------------------- 5.2/10.5 MB 106.3 kB/s eta 0:00:50\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   --------------------- ------------------ 5.5/10.5 MB 109.8 kB/s eta 0:00:46\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 108.6 kB/s eta 0:00:44\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 108.6 kB/s eta 0:00:44\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 108.6 kB/s eta 0:00:44\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 108.6 kB/s eta 0:00:44\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 108.6 kB/s eta 0:00:44\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 108.6 kB/s eta 0:00:44\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 108.6 kB/s eta 0:00:44\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 108.6 kB/s eta 0:00:44\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 108.6 kB/s eta 0:00:44\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 108.6 kB/s eta 0:00:44\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 108.6 kB/s eta 0:00:44\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 108.6 kB/s eta 0:00:44\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 108.6 kB/s eta 0:00:44\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 108.6 kB/s eta 0:00:44\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 108.6 kB/s eta 0:00:44\n",
      "   ----------------------- ---------------- 6.0/10.5 MB 107.3 kB/s eta 0:00:42\n",
      "   ----------------------- ---------------- 6.0/10.5 MB 107.3 kB/s eta 0:00:42\n",
      "   ----------------------- ---------------- 6.0/10.5 MB 107.3 kB/s eta 0:00:42\n",
      "   ----------------------- ---------------- 6.0/10.5 MB 107.3 kB/s eta 0:00:42\n",
      "   ----------------------- ---------------- 6.0/10.5 MB 107.3 kB/s eta 0:00:42\n",
      "   ----------------------- ---------------- 6.0/10.5 MB 107.3 kB/s eta 0:00:42\n",
      "   ----------------------- ---------------- 6.0/10.5 MB 107.3 kB/s eta 0:00:42\n",
      "   ----------------------- ---------------- 6.0/10.5 MB 107.3 kB/s eta 0:00:42\n",
      "   ----------------------- ---------------- 6.0/10.5 MB 107.3 kB/s eta 0:00:42\n",
      "   ----------------------- ---------------- 6.0/10.5 MB 107.3 kB/s eta 0:00:42\n",
      "   ----------------------- ---------------- 6.0/10.5 MB 107.3 kB/s eta 0:00:42\n",
      "   ----------------------- ---------------- 6.0/10.5 MB 107.3 kB/s eta 0:00:42\n",
      "   ------------------------ --------------- 6.3/10.5 MB 105.2 kB/s eta 0:00:40\n",
      "   ------------------------ --------------- 6.3/10.5 MB 105.2 kB/s eta 0:00:40\n",
      "   ------------------------ --------------- 6.3/10.5 MB 105.2 kB/s eta 0:00:40\n",
      "   ------------------------ --------------- 6.3/10.5 MB 105.2 kB/s eta 0:00:40\n",
      "   ------------------------ --------------- 6.3/10.5 MB 105.2 kB/s eta 0:00:40\n",
      "   ------------------------ --------------- 6.3/10.5 MB 105.2 kB/s eta 0:00:40\n",
      "   ------------------------ --------------- 6.3/10.5 MB 105.2 kB/s eta 0:00:40\n",
      "   ------------------------ --------------- 6.3/10.5 MB 105.2 kB/s eta 0:00:40\n",
      "   ------------------------- -------------- 6.6/10.5 MB 108.3 kB/s eta 0:00:37\n",
      "   ------------------------- -------------- 6.6/10.5 MB 108.3 kB/s eta 0:00:37\n",
      "   ------------------------- -------------- 6.6/10.5 MB 108.3 kB/s eta 0:00:37\n",
      "   ------------------------- -------------- 6.6/10.5 MB 108.3 kB/s eta 0:00:37\n",
      "   ------------------------- -------------- 6.6/10.5 MB 108.3 kB/s eta 0:00:37\n",
      "   -------------------------- ------------- 6.8/10.5 MB 115.6 kB/s eta 0:00:32\n",
      "   -------------------------- ------------- 6.8/10.5 MB 115.6 kB/s eta 0:00:32\n",
      "   -------------------------- ------------- 6.8/10.5 MB 115.6 kB/s eta 0:00:32\n",
      "   -------------------------- ------------- 6.8/10.5 MB 115.6 kB/s eta 0:00:32\n",
      "   -------------------------- ------------- 6.8/10.5 MB 115.6 kB/s eta 0:00:32\n",
      "   -------------------------- ------------- 6.8/10.5 MB 115.6 kB/s eta 0:00:32\n",
      "   -------------------------- ------------- 6.8/10.5 MB 115.6 kB/s eta 0:00:32\n",
      "   -------------------------- ------------- 6.8/10.5 MB 115.6 kB/s eta 0:00:32\n",
      "   -------------------------- ------------- 6.8/10.5 MB 115.6 kB/s eta 0:00:32\n",
      "   -------------------------- ------------- 6.8/10.5 MB 115.6 kB/s eta 0:00:32\n",
      "   -------------------------- ------------- 6.8/10.5 MB 115.6 kB/s eta 0:00:32\n",
      "   --------------------------- ------------ 7.1/10.5 MB 115.7 kB/s eta 0:00:30\n",
      "   --------------------------- ------------ 7.1/10.5 MB 115.7 kB/s eta 0:00:30\n",
      "   --------------------------- ------------ 7.1/10.5 MB 115.7 kB/s eta 0:00:30\n",
      "   --------------------------- ------------ 7.1/10.5 MB 115.7 kB/s eta 0:00:30\n",
      "   --------------------------- ------------ 7.1/10.5 MB 115.7 kB/s eta 0:00:30\n",
      "   --------------------------- ------------ 7.1/10.5 MB 115.7 kB/s eta 0:00:30\n",
      "   --------------------------- ------------ 7.1/10.5 MB 115.7 kB/s eta 0:00:30\n",
      "   --------------------------- ------------ 7.1/10.5 MB 115.7 kB/s eta 0:00:30\n",
      "   --------------------------- ------------ 7.1/10.5 MB 115.7 kB/s eta 0:00:30\n",
      "   ---------------------------- ----------- 7.3/10.5 MB 115.1 kB/s eta 0:00:28\n",
      "   ---------------------------- ----------- 7.3/10.5 MB 115.1 kB/s eta 0:00:28\n",
      "   ---------------------------- ----------- 7.3/10.5 MB 115.1 kB/s eta 0:00:28\n",
      "   ---------------------------- ----------- 7.3/10.5 MB 115.1 kB/s eta 0:00:28\n",
      "   ---------------------------- ----------- 7.3/10.5 MB 115.1 kB/s eta 0:00:28\n",
      "   ---------------------------- ----------- 7.3/10.5 MB 115.1 kB/s eta 0:00:28\n",
      "   ---------------------------- ----------- 7.3/10.5 MB 115.1 kB/s eta 0:00:28\n",
      "   ---------------------------- ----------- 7.3/10.5 MB 115.1 kB/s eta 0:00:28\n",
      "   ---------------------------- ----------- 7.3/10.5 MB 115.1 kB/s eta 0:00:28\n",
      "   ----------------------------- ---------- 7.6/10.5 MB 107.8 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 7.6/10.5 MB 107.8 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 7.6/10.5 MB 107.8 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 7.6/10.5 MB 107.8 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 7.6/10.5 MB 107.8 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 7.6/10.5 MB 107.8 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 7.6/10.5 MB 107.8 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 7.6/10.5 MB 107.8 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 7.6/10.5 MB 107.8 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 7.6/10.5 MB 107.8 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 7.6/10.5 MB 107.8 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 7.6/10.5 MB 107.8 kB/s eta 0:00:27\n",
      "   ------------------------------ --------- 7.9/10.5 MB 107.0 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 7.9/10.5 MB 107.0 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 7.9/10.5 MB 107.0 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 7.9/10.5 MB 107.0 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 7.9/10.5 MB 107.0 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 7.9/10.5 MB 107.0 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 7.9/10.5 MB 107.0 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 7.9/10.5 MB 107.0 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 7.9/10.5 MB 107.0 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 7.9/10.5 MB 107.0 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 7.9/10.5 MB 107.0 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 7.9/10.5 MB 107.0 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 7.9/10.5 MB 107.0 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 7.9/10.5 MB 107.0 kB/s eta 0:00:25\n",
      "   ------------------------------- -------- 8.1/10.5 MB 112.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 8.1/10.5 MB 112.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 8.1/10.5 MB 112.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 8.1/10.5 MB 112.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 8.1/10.5 MB 112.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 8.1/10.5 MB 112.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 8.1/10.5 MB 112.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 8.1/10.5 MB 112.3 kB/s eta 0:00:21\n",
      "   -------------------------------- ------- 8.4/10.5 MB 115.5 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 8.4/10.5 MB 115.5 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 8.4/10.5 MB 115.5 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 8.4/10.5 MB 115.5 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 8.4/10.5 MB 115.5 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 8.4/10.5 MB 115.5 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 8.4/10.5 MB 115.5 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 8.4/10.5 MB 115.5 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 8.4/10.5 MB 115.5 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 8.4/10.5 MB 115.5 kB/s eta 0:00:18\n",
      "   --------------------------------- ------ 8.7/10.5 MB 116.3 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 8.7/10.5 MB 116.3 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 8.7/10.5 MB 116.3 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 8.7/10.5 MB 116.3 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 8.7/10.5 MB 116.3 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 8.7/10.5 MB 116.3 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 8.7/10.5 MB 116.3 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 8.7/10.5 MB 116.3 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 8.7/10.5 MB 116.3 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 8.7/10.5 MB 116.3 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 8.7/10.5 MB 116.3 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 8.7/10.5 MB 116.3 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 8.7/10.5 MB 116.3 kB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 118.3 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 118.3 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 118.3 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 118.3 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 118.3 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 118.3 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 118.3 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 118.3 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 118.3 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 118.3 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 118.3 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 118.3 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 118.3 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 118.3 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 118.3 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 118.3 kB/s eta 0:00:14\n",
      "   ----------------------------------- ---- 9.2/10.5 MB 114.1 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 9.2/10.5 MB 114.1 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 9.2/10.5 MB 114.1 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 9.2/10.5 MB 114.1 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 9.2/10.5 MB 114.1 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 9.2/10.5 MB 114.1 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 9.2/10.5 MB 114.1 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 9.2/10.5 MB 114.1 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 9.2/10.5 MB 114.1 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 9.2/10.5 MB 114.1 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 9.2/10.5 MB 114.1 kB/s eta 0:00:12\n",
      "   ------------------------------------ --- 9.4/10.5 MB 117.3 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 9.4/10.5 MB 117.3 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 9.4/10.5 MB 117.3 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 9.4/10.5 MB 117.3 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 9.4/10.5 MB 117.3 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 9.4/10.5 MB 117.3 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 9.4/10.5 MB 117.3 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 9.4/10.5 MB 117.3 kB/s eta 0:00:09\n",
      "   ------------------------------------- -- 9.7/10.5 MB 120.5 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 9.7/10.5 MB 120.5 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 9.7/10.5 MB 120.5 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 9.7/10.5 MB 120.5 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 9.7/10.5 MB 120.5 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 9.7/10.5 MB 120.5 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 9.7/10.5 MB 120.5 kB/s eta 0:00:07\n",
      "   -------------------------------------- - 10.0/10.5 MB 123.6 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 10.0/10.5 MB 123.6 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 10.0/10.5 MB 123.6 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 10.0/10.5 MB 123.6 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 10.0/10.5 MB 123.6 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 10.0/10.5 MB 123.6 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 10.0/10.5 MB 123.6 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 10.0/10.5 MB 123.6 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 10.0/10.5 MB 123.6 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 10.0/10.5 MB 123.6 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 10.0/10.5 MB 123.6 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 10.0/10.5 MB 123.6 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 10.0/10.5 MB 123.6 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 10.0/10.5 MB 123.6 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 10.0/10.5 MB 123.6 kB/s eta 0:00:05\n",
      "   ---------------------------------------  10.2/10.5 MB 113.7 kB/s eta 0:00:03\n",
      "   ---------------------------------------  10.2/10.5 MB 113.7 kB/s eta 0:00:03\n",
      "   ---------------------------------------  10.2/10.5 MB 113.7 kB/s eta 0:00:03\n",
      "   ---------------------------------------  10.2/10.5 MB 113.7 kB/s eta 0:00:03\n",
      "   ---------------------------------------  10.2/10.5 MB 113.7 kB/s eta 0:00:03\n",
      "   ---------------------------------------  10.2/10.5 MB 113.7 kB/s eta 0:00:03\n",
      "   ---------------------------------------  10.2/10.5 MB 113.7 kB/s eta 0:00:03\n",
      "   ---------------------------------------  10.2/10.5 MB 113.7 kB/s eta 0:00:03\n",
      "   ---------------------------------------  10.2/10.5 MB 113.7 kB/s eta 0:00:03\n",
      "   ---------------------------------------  10.2/10.5 MB 113.7 kB/s eta 0:00:03\n",
      "   ---------------------------------------  10.2/10.5 MB 113.7 kB/s eta 0:00:03\n",
      "   ---------------------------------------- 10.5/10.5 MB 111.0 kB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.32.4-py3-none-any.whl (512 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.4 MB 223.9 kB/s eta 0:00:09\n",
      "   -------- ------------------------------- 0.5/2.4 MB 223.9 kB/s eta 0:00:09\n",
      "   -------- ------------------------------- 0.5/2.4 MB 223.9 kB/s eta 0:00:09\n",
      "   -------- ------------------------------- 0.5/2.4 MB 223.9 kB/s eta 0:00:09\n",
      "   -------- ------------------------------- 0.5/2.4 MB 223.9 kB/s eta 0:00:09\n",
      "   -------- ------------------------------- 0.5/2.4 MB 223.9 kB/s eta 0:00:09\n",
      "   -------- ------------------------------- 0.5/2.4 MB 223.9 kB/s eta 0:00:09\n",
      "   -------- ------------------------------- 0.5/2.4 MB 223.9 kB/s eta 0:00:09\n",
      "   -------- ------------------------------- 0.5/2.4 MB 223.9 kB/s eta 0:00:09\n",
      "   ------------ --------------------------- 0.8/2.4 MB 179.5 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 0.8/2.4 MB 179.5 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 0.8/2.4 MB 179.5 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 0.8/2.4 MB 179.5 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 0.8/2.4 MB 179.5 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 0.8/2.4 MB 179.5 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 0.8/2.4 MB 179.5 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 0.8/2.4 MB 179.5 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 0.8/2.4 MB 179.5 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 0.8/2.4 MB 179.5 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 0.8/2.4 MB 179.5 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 0.8/2.4 MB 179.5 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 0.8/2.4 MB 179.5 kB/s eta 0:00:10\n",
      "   ------------ --------------------------- 0.8/2.4 MB 179.5 kB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 134.6 kB/s eta 0:00:11\n",
      "   --------------------- ------------------ 1.3/2.4 MB 169.0 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 1.6/2.4 MB 205.6 kB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 277.0 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.4/2.4 MB 305.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 314.0 kB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.32.4 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.52.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9942feaac94d6b926593e7d825ca17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\utkar\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c7937705504147837ed3d9a76f2c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574adaa555d64abeb2b4634e4df49698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af19a3b62984d98a6808633dcede48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7592, 1010, 2079, 2017, 2066, 5572, 1029, 102]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # You can choose other models too\n",
    "\n",
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7592, 1010, 2079, 2017, 2066, 5572, 1029, 102]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1277"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1272)\n",
      "('your', 1273)\n",
      "('yourself', 1274)\n",
      "('<|endoftext|>', 1275)\n",
      "('<|unk|>', 1276)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1276,\n",
       " 5,\n",
       " 433,\n",
       " 1271,\n",
       " 731,\n",
       " 1104,\n",
       " 26,\n",
       " 1275,\n",
       " 89,\n",
       " 1120,\n",
       " 1085,\n",
       " 1115,\n",
       " 835,\n",
       " 1120,\n",
       " 1276,\n",
       " 9]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
      "Downloading tiktoken-0.9.0-cp312-cp312-win_amd64.whl (894 kB)\n",
      "   ---------------------------------------- 0.0/894.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 894.9/894.9 kB 6.7 MB/s eta 0:00:00\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.9.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size for GPT2 is: 50257\n",
      "The vocabulary size for GPT3 is: 50281\n",
      "The vocabulary size for GPT4 is: 100277\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Initialize the encodings for GPT-2, GPT-3, and GPT-4\n",
    "encodings = {\n",
    "    \"gpt2\": tiktoken.get_encoding(\"gpt2\"),\n",
    "    \"gpt3\": tiktoken.get_encoding(\"p50k_base\"),  # Commonly associated with GPT-3 models\n",
    "    \"gpt4\": tiktoken.get_encoding(\"cl100k_base\")  # Used for GPT-4 and later versions\n",
    "}\n",
    "\n",
    "# Get the vocabulary size for each encoding\n",
    "vocab_sizes = {model: encoding.n_vocab for model, encoding in encodings.items()}\n",
    "\n",
    "# Print the vocabulary sizes\n",
    "for model, size in vocab_sizes.items():\n",
    "    print(f\"The vocabulary size for {model.upper()} is: {size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'the-verdict.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe-verdict.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      2\u001b[0m     raw_text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      4\u001b[0m enc_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(raw_text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'the-verdict.txt'"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 4 #length of the input\n",
    "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens) \n",
    "#to predict the next word in the sequence. \n",
    "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer(torch.tensor([3])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer(input_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D plot with vectors from origin to each point, using different colors\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Define a list of colors for the vectors\n",
    "colors = ['r', 'g', 'b', 'c', 'm', 'y']\n",
    "\n",
    "# Plot each vector with a different color and annotate with the corresponding word\n",
    "for (x, y, z, word, color) in zip(x_coords, y_coords, z_coords, words, colors):\n",
    "    # Draw vector from origin to the point (x, y, z) with specified color and smaller arrow length ratio\n",
    "    ax.quiver(0, 0, 0, x, y, z, color=color, arrow_length_ratio=0.05)\n",
    "    ax.text(x, y, z, word, fontsize=10, color=color)\n",
    "\n",
    "# Set labels for axes\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "# Set plot limits to keep arrows within the plot boundaries\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_zlim([0, 1])\n",
    "\n",
    "plt.title('3D Plot of Word Embeddings with Colored Vectors')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs[1]  # 2nd input token is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "inputs2 = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55], # step     (x^6)\n",
    "   [0.4419, 0.6515, 0.5683]]\n",
    ")\n",
    "\n",
    "# Corresponding words\n",
    "words2 = ['Your', 'journey', 'starts', 'with', 'one', 'step', 'journey-context']\n",
    "\n",
    "# Extract x, y, z coordinates\n",
    "x_coords = inputs2[:, 0].numpy()\n",
    "y_coords = inputs2[:, 1].numpy()\n",
    "z_coords = inputs2[:, 2].numpy()\n",
    "\n",
    "# Create 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each point and annotate with corresponding word\n",
    "for x, y, z, word in zip(x_coords, y_coords, z_coords, words2):\n",
    "    ax.scatter(x, y, z)\n",
    "    ax.text(x, y, z, word, fontsize=10)\n",
    "\n",
    "# Set labels for axes\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "plt.title('3D Plot of Word Embeddings')\n",
    "plt.show()\n",
    "\n",
    "# Create 3D plot with vectors from origin to each point, using different colors\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Define a list of colors for the vectors\n",
    "colors = ['r', 'g', 'b', 'c', 'm', 'y', 'r']\n",
    "\n",
    "# Plot each vector with a different color and annotate with the corresponding word\n",
    "for (x, y, z, word, color) in zip(x_coords, y_coords, z_coords, words2, colors):\n",
    "    # Draw vector from origin to the point (x, y, z) with specified color and smaller arrow length ratio\n",
    "    ax.quiver(0, 0, 0, x, y, z, color=color, arrow_length_ratio=0.05)\n",
    "    ax.text(x, y, z, word, fontsize=10, color=color)\n",
    "\n",
    "# Set labels for axes\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "# Set plot limits to keep arrows within the plot boundaries\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_zlim([0, 1])\n",
    "\n",
    "plt.title('3D Plot of Word Embeddings with Colored Vectors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] #A\n",
    "d_in = inputs.shape[1] #B\n",
    "d_out = 2 #C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "queries = inputs @ W_query\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "print(\"queries.shape:\", queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_2 = keys[1] #A\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = queries @ keys.T # omega\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the tensor\n",
    "tensor = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
    "\n",
    "# Apply softmax without scaling\n",
    "softmax_result = torch.softmax(tensor, dim=-1)\n",
    "print(\"Softmax without scaling:\", softmax_result)\n",
    "\n",
    "# Multiply the tensor by 8 and then apply softmax\n",
    "scaled_tensor = tensor * 8\n",
    "softmax_scaled_result = torch.softmax(scaled_tensor, dim=-1)\n",
    "print(\"Softmax after scaling (tensor * 8):\", softmax_scaled_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to compute variance before and after scaling\n",
    "def compute_variance(dim, num_trials=1000):\n",
    "    dot_products = []\n",
    "    scaled_dot_products = []\n",
    "\n",
    "    # Generate multiple random vectors and compute dot products\n",
    "    for _ in range(num_trials):\n",
    "        q = np.random.randn(dim)\n",
    "        k = np.random.randn(dim)\n",
    "        \n",
    "        # Compute dot product\n",
    "        dot_product = np.dot(q, k)\n",
    "        dot_products.append(dot_product)\n",
    "        \n",
    "        # Scale the dot product by sqrt(dim)\n",
    "        scaled_dot_product = dot_product / np.sqrt(dim)\n",
    "        scaled_dot_products.append(scaled_dot_product)\n",
    "    \n",
    "    # Calculate variance of the dot products\n",
    "    variance_before_scaling = np.var(dot_products)\n",
    "    variance_after_scaling = np.var(scaled_dot_products)\n",
    "\n",
    "    return variance_before_scaling, variance_after_scaling\n",
    "\n",
    "# For dimension 5\n",
    "variance_before_5, variance_after_5 = compute_variance(5)\n",
    "print(f\"Variance before scaling (dim=5): {variance_before_5}\")\n",
    "print(f\"Variance after scaling (dim=5): {variance_after_5}\")\n",
    "\n",
    "# For dimension 20\n",
    "variance_before_100, variance_after_100 = compute_variance(100)\n",
    "print(f\"Variance before scaling (dim=100): {variance_before_100}\")\n",
    "print(f\"Variance after scaling (dim=100): {variance_after_100}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = sa_v2.W_query(inputs) #A\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(context_length, context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.triu(torch.ones(context_length, context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = torch.ones(6, 6) #B\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) #A\n",
    "example = torch.ones(6, 6) #B\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens = 6\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# Define the tensor with 3 rows and 6 columns\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
    "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) \n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 6\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5) #A\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# Some sample data\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
    "            GELU(), ## Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GPT_CONFIG_124M[\"emb_dim\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768) #A\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123) # specify random seed for the initial weights for reproducibility\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
    "            GELU(), ## Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        # 2*4*768\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "        # 2*4*768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768) #A\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
    "            GELU(), ## Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        # 2*4*768\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "        # 2*4*768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size_bytes = total_params * 4 #A\n",
    "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "\n",
    "    ###Input batch:\n",
    " ###tensor([[6109, 3626, 6100,  345],\n",
    "        ##[6109, 1110, 6622,  257]])\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() #A\n",
    "#model = GPTModel(GPT_CONFIG_124M)\n",
    "out = generate_text_simple(\n",
    "model=model,\n",
    "idx=encoded_tensor,\n",
    "max_new_tokens=6,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "\n",
    "\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 100 characters\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last 100 characters\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "next_token_logits2 = next_token_logits/0.1\n",
    "\n",
    "next_token_logits3 = next_token_logits/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = torch.softmax(next_token_logits2, dim=0)\n",
    "\n",
    "print(probas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = torch.softmax(next_token_logits3, dim=0)\n",
    "\n",
    "print(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "\n",
    "print(probas)\n",
    "\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "print(next_token_id)\n",
    "\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "\n",
    "##Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow>=2.15.0 tqdm>=4.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"tqdm version:\", tqdm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_download3 import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
